from pyspark import SparkContext as sc

data = sc.textFile("./resources/dataset_TIST2015_Cities.txt")
print(type(data))


data = sc.textFile("./resources/dataset_TIST2015.tsv")
data1 = sc.textFile("./resources/dataset_TIST2015_Cities.txt")

def toCSVLine(data):
  return ','.join(str(d) for d in data)


parallelize(c, numSlices=None)
Distribute a local Python collection to form an RDD. Using xrange is recommended if the input represents a range for performance.